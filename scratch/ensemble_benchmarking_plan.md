# Ensemble Aggregation Strategy Benchmarking Plan

## Context & Problem Statement

This project implements a Metaculus forecasting bot with model ensembling. Currently there are two key requirements:

1. **Production Usage**: Deploy a 3-model ensemble (GPT-5 + Claude Sonnet 4 + DeepSeek R1) with configurable aggregation (mean vs median)
2. **Benchmarking**: Test different ensemble compositions AND aggregation strategies to optimize performance

### Current Problem
The benchmarking code in `community_benchmark.py:267-288` incorrectly generates model×strategy combinations like:
- `qwen3-235b_mean` vs `qwen3-235b_median` 

This applies aggregation to a **single model's prediction**, which is meaningless since there's only one value to aggregate.

### Correct Approach  
Should generate ensemble×strategy combinations like:
- `qwen3-235b` (baseline single model)
- `qwen3_glm_mean` vs `qwen3_glm_median` (2-model ensemble with different aggregation)
- `full_ensemble_mean` vs `full_ensemble_median` (3-model ensemble like production)

## Architecture Overview

### Key Files & Components

**Core Bot Implementation:**
- `main.py`: `TemplateForecaster` class with `aggregation_strategy` parameter
- `metaculus_bot/aggregation_strategies.py`: `AggregationStrategy` enum and aggregation functions
- `metaculus_bot/llm_configs.py`: Production model configurations (`FORECASTER_LLMS` has 3 models)

**Benchmarking Infrastructure:**
- `community_benchmark.py`: Benchmark runner that generates bot configurations
- `metaculus_bot/correlation_analysis.py`: Post-benchmark analysis for ensemble optimization
- `analyze_correlations.py`: Standalone script for analyzing existing benchmark results

**Testing:**
- `tests/test_aggregation_strategies.py`: Unit tests for aggregation functions
- `tests/test_aggregation.py`: Integration tests for bot aggregation behavior
- `tests/test_community_benchmark.py`: Benchmark configuration tests

## Implementation Plan

### Phase 1: Keep Core Aggregation Support ✅ 
**Status: Already implemented correctly**

The core aggregation infrastructure is correct and needed for production:
- `TemplateForecaster` accepts `aggregation_strategy: AggregationStrategy` parameter
- `_aggregate_predictions()` method applies strategy to multiple model predictions
- Supports binary, numeric, and multiple choice question types

### Phase 2: Fix Benchmark Configuration Generation ✅ 
**Status: Completed - MAJOR ARCHITECTURAL CHANGE**

**Target File: `community_benchmark.py:289-320`**

**Old Approach (Incorrect):**
Manual ensemble×strategy combinations in benchmark configuration

**New Approach (Correct):**
1. **Benchmark individual models only** - Test each model separately
2. **Generate ensembles post-hoc** - Use `CorrelationAnalyzer.find_optimal_ensembles()` to automatically generate all possible combinations

**New Individual Model Configuration:**
```python
# Individual model configurations for benchmarking  
# Test each model separately - ensembles will be generated post-hoc by analyze_correlations.py
individual_models = [
    {"name": "qwen3-235b", "forecaster": qwen3_model},
    {"name": "glm-4.5", "forecaster": glm_model},
    # Additional models commented for cost control...
]

# Generate individual model bots - ensembles generated by CorrelationAnalyzer.find_optimal_ensembles()
bots = []
for model_config in individual_models:
    bot = TemplateForecaster(
        **BENCHMARK_BOT_CONFIG,
        aggregation_strategy=AggregationStrategy.MEAN,  # Default, not used for single models
        llms={
            "forecasters": [model_config["forecaster"]],
            **DEFAULT_HELPER_LLMS,
        },
        max_concurrent_research=batch_size,
        research_cache=research_cache,
    )
    bot.name = model_config["name"]
    bots.append(bot)
```

**Benefits of New Approach:**
- **No boilerplate**: Automatically generates all ensemble combinations
- **Flexible**: Can test any ensemble size and aggregation strategy
- **Efficient**: Only benchmarks individual models once
- **Comprehensive**: Tests all possible combinations via `analyze_correlations.py`

### Phase 3: Preserve Test Model Configurations ✅ 
**Status: Completed**

**Target: `community_benchmark.py:246-265`**

Keep the commented-out models in `BASE_MODEL_CONFIGS` for future testing:
```python
BASE_MODEL_CONFIGS = [
    {"name": "qwen3-235b", "forecasters": [...]},
    {"name": "glm-4.5", "forecasters": [...]},
    # Keep these commented for cost control during development:
    # {"name": "gpt-5", "forecasters": [...]},
    # {"name": "claude-sonnet-4", "forecasters": [...]},
    # {"name": "deepseek-r1", "forecasters": [...]},
]
```

### Phase 4: Update Analysis Framework ✅ 
**Status: Completed**

**Target: `metaculus_bot/correlation_analysis.py`**

The correlation analyzer already works correctly for ensemble analysis, but may need minor enhancements:

1. **Model Name Extraction**: Update `_extract_model_name()` to handle ensemble names like `qwen3_glm_mean`
2. **Ensemble Grouping**: Add logic to group related ensemble configurations for comparison
3. **Aggregation Strategy Reporting**: Include aggregation method in correlation reports

### Phase 5: Update Tests ✅ 
**Status: Completed**

**Files to update:**
- `tests/test_community_benchmark.py`: Update expected bot count and names
- `tests/test_aggregation.py`: Verify ensemble aggregation still works
- Add integration test for ensemble×strategy matrix generation

**Expected Test Updates:**
```python
def test_benchmark_bot_generation():
    # Should generate: 2 single models + 2×2 ensemble configurations  
    # = 2 + 4 = 6 total bots
    expected_names = [
        "qwen3-235b", "glm-4.5",           # Single models
        "qwen3_glm_mean", "qwen3_glm_median",  # 2-model ensemble
        "full_ensemble_mean", "full_ensemble_median"  # 3-model ensemble  
    ]
```

## Expected Outcomes

### Benchmark Results
After implementation, benchmark runs will generate meaningful comparisons:

1. **Baseline Performance**: How do individual models perform?
2. **Ensemble Value**: Does combining models improve performance?
3. **Aggregation Strategy**: For each ensemble size, does mean or median work better?
4. **Cost-Performance Trade-offs**: What's the optimal ensemble size for budget X?

### Production Optimization
Analysis will inform production ensemble configuration:
- Which models to include in `FORECASTER_LLMS`
- Whether to use `AggregationStrategy.MEAN` or `AggregationStrategy.MEDIAN`
- Optimal ensemble size for cost/performance ratio

### Correlation Analysis
Enhanced correlation reports will show:
- Inter-model correlations for diversity assessment
- Best ensemble compositions for maximum diversity
- Aggregation strategy recommendations per ensemble type

## Implementation Priority

1. **High Priority**: Fix benchmark configuration generation (Phase 2)
2. **Medium Priority**: Update tests to match new configuration (Phase 5)  
3. **Low Priority**: Enhance correlation analysis reporting (Phase 4)

## Success Criteria

✅ **Production Support**: 3-model ensemble in `llm_configs.py` works with configurable aggregation
✅ **Meaningful Benchmarks**: Generate ensemble×strategy combinations, not model×strategy  
✅ **Cost Control**: Single models tested once, ensembles tested with both aggregation strategies
✅ **Analysis Ready**: Results suitable for correlation analysis and ensemble optimization

## Notes

- Preserve commented models in configurations for future use
- Maintain backward compatibility with existing production configurations
- Ensure cost control during development (use cheap models for testing)
- Generate comprehensive benchmark results for ensemble optimization analysis